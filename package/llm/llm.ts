import OpenAI from "openai";
import { LLM_OPS_CONFIG } from "llm-ops/utils/index";
/** OpenAIèŠå¤©è¯·æ±‚æ¶ˆæ¯çš„ç±»å‹æ•°ç»„ */
export type messagesType = OpenAI.Chat.CreateChatCompletionRequestMessage[];
/** å•ä¸€çš„OpenAIèŠå¤©è¯·æ±‚æ¶ˆæ¯ç±»å‹ */
export type messageType = OpenAI.Chat.CreateChatCompletionRequestMessage;
/** OpenAIèŠå¤©å®Œæˆçš„æ¶ˆæ¯æ•°ç»„ç±»å‹ */
export type resMessagesType = OpenAI.Chat.Completions.ChatCompletion.Choice[];
/** FunctionCallçš„æ¥å£ */
export interface FunctionCallOption {
  /**
   * The name of the function to call.
   */
  name: string;
}
export interface FunctionInterface {
  /**
   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain
   * underscores and dashes, with a maximum length of 64.
   */
  name: string;

  /**
   * The parameters the functions accepts, described as a JSON Schema object. See the
   * [guide](https://platform.openai.com/docs/guides/gpt/function-calling) for
   * examples, and the
   * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for
   * documentation about the format.
   *
   * To describe a function that accepts no parameters, provide the value
   * `{"type": "object", "properties": {}}`.
   */
  parameters: Record<string, unknown>;

  /**
   * A description of what the function does, used by the model to choose when and
   * how to call the function.
   */
  description?: string;
}
/** FunctionInterfaceç±»å‹æ•°ç»„ */
export type functionsType = FunctionInterface[];
/** function_callç±»å‹ */
export type function_callType = "none" | "auto" | FunctionCallOption;
/** OpenAIèŠå¤©å®Œæˆæ¶ˆæ¯ä¸­çš„FunctionCallç±»å‹ */
export type messageFunctionCallType =
  OpenAI.Chat.ChatCompletionMessage.FunctionCall;
/** OpenAIèŠå¤©å®Œæˆç±»å‹ */
export type chatCompletionType = OpenAI.Chat.Completions.ChatCompletion;
/** LLMç±»å‹å®šä¹‰ä¸ºOpenAIç±»å‹ */
export type llmType = OpenAI;
/** OpenAIèŠå¤©å‚æ•°ç±»å‹ */
export type chatParamsType = OpenAI.Chat.CompletionCreateParams;
/** OpenAIå†…å®¹å®¡æŸ¥å“åº”ç±»å‹ */
export type resModerationType = OpenAI.ModerationCreateResponse;
/** OpenAIåˆ›å»ºåµŒå…¥å“åº”ç±»å‹ */
export type resEmbeddingType = OpenAI.CreateEmbeddingResponse;
/** åˆ›å»ºLLMçš„å‚æ•°æ¥å£ */
export interface createLLMSchema {
  /** HeliconeæˆæƒAPIå¯†é’¥ */
  HELICONE_AUTH_API_KEY?: string;
  /** OpenAI APIå¯†é’¥ */
  OPENAI_API_KEY?: string;
  /** æ¨¡å‹åç§°é€‰é¡¹ */
  modelName?:
    | (string & object)
    | "gpt-4"
    | "gpt-4-0314"
    | "gpt-4-0613"
    | "gpt-4-32k"
    | "gpt-4-32k-0314"
    | "gpt-4-32k-0613"
    | "gpt-3.5-turbo"
    | "gpt-3.5-turbo-16k"
    | "gpt-3.5-turbo-0301"
    | "gpt-3.5-turbo-0613"
    | "gpt-3.5-turbo-16k-0613";
  temperature?: number;
  choice_num?: number | 1;
  stop?: string | null | string[];
  cache?: boolean;
  user?: string;
  history?: messagesType;
  tokens?: number;
}
/** èŠå¤©å‚æ•°æ¥å£ */
export interface ChatSchema {
  function_call?: function_callType;
  /** æ¶ˆæ¯æ•°ç»„ */
  messages: messagesType;
  functions?: functionsType;
}
/**
 * LLMç±»ä»£è¡¨ä¸€ä¸ªä¸LLM APIè¿›è¡Œäº¤äº’çš„å®¢æˆ·ç«¯ã€‚
 */
export class LLM {
  llm: llmType;
  tokens: number;
  messages: messagesType;
  modelName?:
    | (string & object)
    | "gpt-4"
    | "gpt-4-0314"
    | "gpt-4-0613"
    | "gpt-4-32k"
    | "gpt-4-32k-0314"
    | "gpt-4-32k-0613"
    | "gpt-3.5-turbo"
    | "gpt-3.5-turbo-16k"
    | "gpt-3.5-turbo-0301"
    | "gpt-3.5-turbo-0613"
    | "gpt-3.5-turbo-16k-0613";
  temperature?: number;
  function_call?: function_callType;
  functions?: functionsType;
  choice_num?: number | 1;
  stop?: string | null | string[];
  cache?: boolean;
  roleToColor = {
    system: "red",
    user: "green",
    assistant: "blue",
    function: "magenta",
  };
  user: string;

  /**
   * æ„é€ ä¸€ä¸ªLLMå®¢æˆ·ç«¯å®ä¾‹ã€‚
   * @param params - åŒ…å«åˆ›å»ºLLMå®¢æˆ·ç«¯æ‰€éœ€çš„å‚æ•°ã€‚
   */
  constructor(params: createLLMSchema) {
    const {
      HELICONE_AUTH_API_KEY = undefined,
      OPENAI_API_KEY = undefined,
      modelName,
      temperature,
      choice_num,
      stop,
      cache,
      user,
      history,
      tokens,
    } = params;
    this.llm = this._createLLM({ HELICONE_AUTH_API_KEY, OPENAI_API_KEY });
    this.tokens = tokens || 0;
    this.messages = history || [];
    this.modelName = modelName || "gpt-3.5-turbo-0613";
    this.temperature = temperature || 0.7;
    this.choice_num = choice_num || 1;
    this.stop = stop || null;
    this.cache = cache || true;
    this.user = user || "LLM Ops";
  }
  /**
   * å¯¼å‡ºå†å²æ¶ˆæ¯ã€‚
   * @returns è¿”å›èŠå¤©çš„å†å²æ¶ˆæ¯ã€‚
   */
  exportHistory() {
    return this.messages;
  }

  private _createLLM({
    HELICONE_AUTH_API_KEY = undefined,
    OPENAI_API_KEY = undefined,
  }: createLLMSchema): llmType {
    const openAIApiKey = OPENAI_API_KEY || LLM_OPS_CONFIG.OPENAI_API_KEY;
    if (!openAIApiKey) {
      this.missingEnvironmentVariable(
        "OPENAI_API_KEY Missing! ğŸ˜… It's not free!",
      );
    }
    const config = !!HELICONE_AUTH_API_KEY
      ? {
          baseURL: "https://oai.hconeai.com/v1",
          defaultHeaders: {
            "Helicone-Auth": `Bearer ${process.env["HELICONE_AUTH_API_KEY"]}`,
          },
        }
      : !!LLM_OPS_CONFIG.OPEN_PATH
      ? LLM_OPS_CONFIG.OPEN_PATH
      : LLM_OPS_CONFIG.HELICONE_AUTH_API_KEY
      ? {
          baseURL: "https://oai.hconeai.com/v1",
          defaultHeaders: {
            "Helicone-Auth": `Bearer ${LLM_OPS_CONFIG.HELICONE_AUTH_API_KEY}`,
          },
        }
      : {};
    return new OpenAI({
      ...config,
      apiKey: openAIApiKey,
    });
  }

  /**
   * æŠ›å‡ºç¼ºå°‘ç¯å¢ƒå˜é‡çš„å¼‚å¸¸ã€‚
   * @param name - ç¼ºå°‘çš„ç¯å¢ƒå˜é‡çš„åç§°ã€‚
   */
  private missingEnvironmentVariable(name: string): never {
    throw new Error(`"Missing environment variable: ${name}`);
  }
  /**
   * ä½¿ç”¨LLMè¿›è¡ŒèŠå¤©ã€‚
   * @param params - åŒ…å«èŠå¤©æ‰€éœ€çš„å‚æ•°ã€‚
   * @returns è¿”å›èŠå¤©çš„å®Œæˆä¿¡æ¯ã€‚
   */
  async chat(params: ChatSchema): Promise<chatCompletionType> {
    const { messages, function_call, functions } = params;
    try {
      if (!messages) {
        throw new Error("messages is required!");
      }
      !!this.cache &&
        this.messages.length != 0 &&
        (this.messages = [...this.messages, ...messages]);
      !this.cache && (this.messages = messages);
      !!this.cache && this.messages.length == 0 && (this.messages = messages);
      !!function_call && (this.function_call = function_call);
      !!functions && (this.functions = functions);
      const params_: chatParamsType = {
        /**
         * A list of messages comprising the conversation so far.
         * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
         */
        messages: this.messages,
        /**
         * ID of the model to use. See the
         * [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table
         * for details on which models work with the Chat API.
         */
        model: this.modelName || "gpt-3.5-turbo-0613",
        /**
         * Controls how the model responds to function calls. "none" means the model does
         * not call a function, and responds to the end-user. "auto" means the model can
         * pick between an end-user or calling a function. Specifying a particular function
         * via `{"name":\ "my_function"}` forces the model to call that function. "none" is
         * the default when no functions are present. "auto" is the default if functions
         * are present.
         */
        function_call: functions && function_call ? function_call : undefined,
        /**
         * A list of functions the model may generate JSON inputs for.
         */
        functions: functions,
        /**
         * How many chat completion choices to generate for each input message.
         */
        n: this.choice_num || 1,
        stop: this.stop,
        stream: false,
        temperature: this.temperature || 0.7,
        user: this.user,
      };
      const res = (await this.llm.chat.completions.create(
        params_,
      )) as chatCompletionType;
      this.tokens += res.usage?.total_tokens || 0;
      !!this.cache &&
        res.choices.length == 1 &&
        this.messages.push(res.choices[0]?.message as messageType);
      return res;
    } catch (err) {
      console.error(err);
      throw err;
    }
  }
  /**
   * é‡å¤æœ€åä¸€æ¬¡çš„èŠå¤©ã€‚
   * @returns è¿”å›é‡å¤çš„èŠå¤©å®Œæˆä¿¡æ¯ã€‚
   */
  async recall(): Promise<chatCompletionType> {
    try {
      if (this.messages.length == 0) {
        throw new Error("can't recall!");
      }
      if (
        !!this.cache &&
        this.messages[this.messages.length - 1]?.role == "assistant"
      ) {
        console.warn("pop assistant message!");
        this.messages.pop();
      }
      const params_: chatParamsType = {
        /**
         * A list of messages comprising the conversation so far.
         * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).
         */
        messages: this.messages,
        /**
         * ID of the model to use. See the
         * [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table
         * for details on which models work with the Chat API.
         */
        model: this.modelName || "gpt-3.5-turbo-0613",
        /**
         * Controls how the model responds to function calls. "none" means the model does
         * not call a function, and responds to the end-user. "auto" means the model can
         * pick between an end-user or calling a function. Specifying a particular function
         * via `{"name":\ "my_function"}` forces the model to call that function. "none" is
         * the default when no functions are present. "auto" is the default if functions
         * are present.
         */
        function_call:
          this.functions && this.function_call ? this.function_call : undefined,
        /**
         * A list of functions the model may generate JSON inputs for.
         */
        functions: this.functions,
        /**
         * How many chat completion choices to generate for each input message.
         */
        n: this.choice_num || 1,
        stop: this.stop,
        stream: false,
        temperature: this.temperature || 0.7,
        user: this.user,
      };
      const res = (await this.llm.chat.completions.create(
        params_,
      )) as chatCompletionType;
      this.tokens += res.usage?.total_tokens || 0;
      !!this.cache &&
        res.choices.length == 1 &&
        this.messages.push(res.choices[0]?.message as messageType);
      return res;
    } catch (err) {
      console.error(err);
      throw err;
    }
  }
  /**
   * å¯¹è¾“å…¥å†…å®¹è¿›è¡Œå®¡æŸ¥ã€‚
   * @param input - éœ€è¦å®¡æŸ¥çš„è¾“å…¥å†…å®¹ã€‚
   * @returns è¿”å›å®¡æŸ¥ç»“æœã€‚
   */
  async moderate(input: string | string[]): Promise<resModerationType> {
    return await this.llm.moderations.create({
      input: input,
      model: "text-moderation-latest",
    });
  }
  /**
   * è·å–è¾“å…¥å†…å®¹çš„åµŒå…¥å‘é‡ã€‚
   * @param input - éœ€è¦åµŒå…¥çš„è¾“å…¥å†…å®¹ã€‚
   * @returns è¿”å›åµŒå…¥ç»“æœã€‚
   */
  async embedding(
    input: string | string[] | number[] | number[][],
  ): Promise<resEmbeddingType> {
    return await this.llm.embeddings.create({
      input: input,
      model: "text-embedding-ada-002",
      user: this.user,
    });
  }
  /**
   * æ—¥å¿—è¾“å‡ºã€‚
   * @param args - è¦è¾“å‡ºçš„å†…å®¹ã€‚
   */
  static log(...args: string[]) {
    console.log(args);
  }
  /**
   * ä¼˜é›…åœ°æ‰“å°è¯·æ±‚æ¶ˆæ¯ã€‚
   * @param messages - éœ€è¦æ‰“å°çš„æ¶ˆæ¯ã€‚
   */
  private prettyPrintReqMessage(messages: messagesType) {
    for (const message of messages) {
      if (message.role === "system") {
        LLM.log(
          `%c system ${message.name ? "(" + message.name + ")" : ""}: ${
            message.content
          } \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      } else if (message.role === "user") {
        LLM.log(
          `%c user: ${message.content} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      } else if (message.role === "assistant" && message.function_call) {
        LLM.log(
          `%c assistant: ${JSON.stringify(message.function_call)} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      } else if (message.role === "assistant" && !message.function_call) {
        LLM.log(
          `%c assistant: ${message.content} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      } else if (message.role === "function") {
        LLM.log(
          `%c function (${
            message.name // response message has not `name`
          }): ${message.content} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      }
    }
  }
  /**
   * ä¼˜é›…åœ°æ‰“å°å“åº”æ¶ˆæ¯ã€‚
   * @param messages - éœ€è¦æ‰“å°çš„æ¶ˆæ¯ã€‚
   */
  private prettyPrintResMessage(messages: resMessagesType) {
    for (const message_ of messages) {
      const message = message_.message;
      if (message.role === "system") {
        LLM.log(
          `%c system: ${message.content} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      } else if (message.role === "user") {
        LLM.log(
          `%c user: ${message.content} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      } else if (message.role === "assistant" && message.function_call) {
        LLM.log(
          `%c assistant: ${JSON.stringify(message.function_call)} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      } else if (message.role === "assistant" && !message.function_call) {
        LLM.log(
          `%c assistant: ${message.content} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      } else if (message.role === "function") {
        LLM.log(
          `%c function : ${message.content} \n`,
          `color: ${this.roleToColor[message.role]}`,
        );
      }
    }
  }
  /**
   * æ‰“å°æ¶ˆæ¯ã€‚
   * @param resMessages - å“åº”æ¶ˆæ¯ã€‚
   * @param reqMessages - è¯·æ±‚æ¶ˆæ¯ã€‚
   */
  printMessage(resMessages?: resMessagesType, reqMessages?: messagesType) {
    !!reqMessages && this.prettyPrintReqMessage(reqMessages);
    !!resMessages && this.prettyPrintResMessage(resMessages);
    !reqMessages && !resMessages && this.prettyPrintReqMessage(this.messages);
  }
}
